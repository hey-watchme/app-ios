# 動画から音声抽出機能 - 実装状況と今後の方針

最終更新: 2025-11-23

---

## 📱 機能概要

カメラロールの動画から音声を抽出し、WatchMeの分析フローに送信する機能。
**動画撮影日時をベースに過去の記録として分析**することで、ユーザーが好きなタイミングで分析データを追加できる。

---

## ✅ 実装完了項目（2025-11-23）

### UI/UX
- ✅ FABボタン（動画アイコン）でカメラロール起動
- ✅ PHPickerViewControllerによる動画選択
- ✅ 確認ダイアログ「この動画を分析しますか？」
- ✅ トーストによる進捗表示（動画読み込み → 音声抽出 → アップロード）
- ✅ プログレスバー（各ステップ0%→100%）

### 技術実装
- ✅ M4A形式で音声抽出（AVAssetExportSession）
- ✅ Vault APIでM4A→WAV自動変換（pydub + ffmpeg）
- ✅ 既存のアップロードフロー統合
- ✅ エラーハンドリング（トースト表示）

### 実装ファイル
- `/ios_watchme_v9/ios_watchme_v9/VideoPicker.swift` - カスタムピッカー実装
- `/ios_watchme_v9/ios_watchme_v9/VideoPickerView.swift` - メイン処理
- `/ios_watchme_v9/ios_watchme_v9/ContentView.swift` - FABボタン
- `/ios_watchme_v9/ios_watchme_v9/Components/FloatingActionButton.swift` - 共通FAB
- `/projects/watchme/api/vault/app.py` - M4A→WAV変換機能

---

## 🎯 音声形式とサーバー対応

### iOS側（クライアント）
- **抽出形式**: M4A（AAC）
- **ファイルサイズ**: 20秒の動画 → 約200KB
- **メリット**: 通信量削減、バッテリー消費削減

### Vault API側（サーバー）
- **受信**: M4A / WAV 両対応（拡張子で自動判定）
- **変換**: M4A → WAV (16kHz, mono, 16-bit) に自動変換
- **使用ライブラリ**: pydub + ffmpeg
- **保存**: S3には常にWAV形式で保存

```python
# app.py (330-342行目)
if file_extension == 'm4a':
    file_content, content_type = convert_m4a_to_wav(file_content, filename)
elif file_extension == 'wav':
    content_type = 'audio/wav'
```

---

## 🚨 現在の課題と今後の方針

### 課題1: 動画撮影時間の扱い

**問題点**:
- 現在、動画をアップロードすると「現在時刻」で記録される
- 実際の動画撮影時刻（例：2日前）が反映されない
- ダッシュボードは「最新」を表示するため、過去の動画分析結果が見つからない

**解決方針**:
1. **動画の撮影日時を取得**（PHAsset.creationDate）
2. **recorded_at に撮影日時を設定**してアップロード
3. **ダッシュボードのUI改善**（日付選択機能の強化）

**技術的実装（iOS）**:
```swift
// PHAssetから撮影日時を取得
if let phAsset = PHAsset.fetchAssets(withLocalIdentifiers: [assetIdentifier], options: nil).firstObject,
   let creationDate = phAsset.creationDate {
    // この日時をrecorded_atとして使用
    uploadRequest.recordedAt = creationDate
}
```

### 課題2: 2つのユーザー体験の統合

**現状の2つのモード**:
1. **録音デバイスモード**（コア機能）
   - 観測対象デバイスのダッシュボード
   - リアルタイム更新、プッシュ通知
   - 継続的なモニタリング

2. **スポット分析モード**（補助機能）
   - 手動録音（1回やって満足、お試し程度）
   - 動画から分析（★ 重要度高い）

**問題点**:
- 手動録音は「お試し」以上の価値がない
- 動画分析は有用だが、「過去のデータ」としてどう扱うかが不明確
- ダッシュボードの情報設計が「リアルタイム」前提

**今後の方針**:
- **手動録音機能は現状維持**（削除しないが、優先度低）
- **動画分析を主要機能として育てる**
- **ダッシュボードの情報設計を見直し**:
  - 日付選択機能の改善
  - 過去データの表示方法
  - タイムライン表示の検討

---

## 🎬 動画分析の将来的な拡張

### 音声だけに限定しない

**現状**: 動画から音声のみ抽出

**将来的な可能性**:
1. **動画サムネイル**
   - 分析結果と一緒にサムネイルを表示
   - 「どの場面か」を視覚的に確認可能

2. **動画内の画像解析**
   - 表情分析（感情推定）
   - シーン認識（場所・状況の把握）
   - 行動認識（座っている、立っている等）

3. **マルチモーダル分析**
   - 音声 + 画像の統合分析
   - より精度の高い感情・行動推定

**技術的な実現可能性**:
- ✅ iOSでサムネイル取得は容易（AVAssetImageGenerator）
- ✅ フレーム画像の抽出も可能
- ⚠️ サーバー側の画像解析APIが必要（新規開発）

---

## 🔧 技術仕様

### 音声形式（サーバー保存時）
- 形式: WAV (Linear PCM)
- サンプリングレート: 16kHz
- チャンネル: 1（モノラル）
- ビット深度: 16bit

### データフロー
```
1. ユーザーが動画選択
2. 確認ダイアログ → 「はい」
3. カメラロール閉じる
4. トースト: 動画読み込み中... (M4A抽出)
5. トースト: 音声を抽出中... (完了)
6. トースト: アップロード中... (Vault APIへ送信)
7. Vault API: M4A → WAV変換
8. S3保存 + Supabaseメタデータ登録
9. Lambda処理 → 分析API実行
10. 分析結果保存 → プッシュ通知
```

---

## 📋 次のマイルストーン

### Phase 1: 撮影時間の反映（最優先）
- [ ] PHAssetから撮影日時取得
- [ ] recorded_at に撮影日時を設定
- [ ] ダッシュボードで過去日付の表示改善
- [ ] 日付選択UIの改善

### Phase 2: 情報設計の見直し
- [ ] ダッシュボードのタイムライン表示
- [ ] 「スポット分析」セクションの追加
- [ ] 手動録音・動画分析結果の整理

### Phase 3: マルチモーダル分析（将来）
- [ ] 動画サムネイル表示
- [ ] フレーム画像の抽出・保存
- [ ] 画像解析APIの開発
- [ ] 音声+画像の統合分析

---

## 🐛 既知の制限事項

1. **動画の長さ**
   - 長尺動画（10分以上）は処理に時間がかかる可能性
   - 現状、ファイルサイズ上限100MB

2. **音声がない動画**
   - 音声トラックがない動画はエラー表示

3. **タイムゾーン**
   - 現在は端末のタイムゾーンを使用
   - 海外で撮影した動画の扱い要検討

---

## 📝 まとめ

### 実装完了
- 動画から音声抽出・分析機能は**ほぼ完成**
- UX改善（確認ダイアログ、トースト表示、プログレスバー）完了
- サーバー側のM4A対応完了

### 今後の課題
1. **最優先**: 動画撮影時間の反映
2. **重要**: ダッシュボードの情報設計見直し
3. **将来**: マルチモーダル分析（音声+画像）

### 方針
- 手動録音機能は現状維持（優先度低）
- **動画分析を主要機能として育てる**
- 音声だけでなく、画像も活用する方向へ
